{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "4mh2MiuVMazk"}, "source": "# TP2 - Market Basket Analysis \nINF8111 - Fouille de donn\u00e9es, \u00c9t\u00e9 2023\n### Membres de l'\u00e9quipe\n    - Marco Mudenge\n    - Matthieu Souttre\n"}, {"cell_type": "markdown", "metadata": {"id": "VmJEz5JEMazl"}, "source": "## Date et directives de remise\nVous remettrez ce fichier nomm\u00e9 TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb dans la bo\u00eete de remise sur moodle. \n\n**Date limite: Juin 04, 23:55**.\n\n\n## Market Basket Analysis\n\nLe *Market Basket Analysis* (MBA) est une technique d'analyse de la fouille de donn\u00e9es qui permet de d\u00e9couvrir les associations entre les produits ou leur regroupement. En explorant des motifs int\u00e9ressants \u00e0 partir d'une vaste collection de donn\u00e9es, le MBA vise \u00e0 comprendre / r\u00e9v\u00e9ler les comportements d'achat des clients en se basant sur la th\u00e9orie selon laquelle si vous avez achet\u00e9 un certain ensemble de produits, vous \u00eates plus (ou moins) susceptible d'acheter un autre groupe de produits. En d'autres termes, le MBA permet aux d\u00e9taillants d'identifier la relation entre les articles que les clients ach\u00e8tent, r\u00e9v\u00e9lant des tendances d'articles souvent achet\u00e9s ensemble.\n\nUne approche largement utilis\u00e9e pour explorer ces motifs consiste \u00e0 construire *** des r\u00e8gles d'association *** telles que\n- **si** achet\u00e9 *ITEM_1* **alors** ach\u00e8tera *ITEM_2* avec **confiance** *X*.\n\nCes associations n'ont pas \u00e0 \u00eatre des r\u00e8gles individuelles. Ils peuvent impliquer de nombreux \u00e9l\u00e9ments. Par exemple, une personne dans un supermarch\u00e9 peut ajouter des \u0153ufs dans son panier, puis le MBA peut sugg\u00e9rer qu'elle ach\u00e8tera \u00e9galement du pain et/ou de la farine:\n\n+ **si**  achet\u00e9 *OEUFS* **alors** ach\u00e8tera [*PAIN* avec confiance *0,2*; *FARINE* avec confiance 0,05].\n\nCependant, si la personne d\u00e9cide maintenant d'ajouter de la farine \u00e0 son panier, la nouvelle r\u00e8gle d'association pourrait \u00eatre comme ci-dessous, sugg\u00e9rant des ingr\u00e9dients pour faire un g\u00e2teau.\n\n+ **si** achet\u00e9 [*OEUFS, FARINE*] **alors** ach\u00e8tera [*SUCRE* avec confiance 0,45; LEVURE avec confiance 0,12; *PAIN* avec confiance *0,03*].\n\nIl existe de nombreux sc\u00e9narios r\u00e9els o\u00f9 le MBA joue un r\u00f4le central dans l'analyse des donn\u00e9es, comme les transactions de supermarch\u00e9, les commandes en ligne ou l'historique des cartes de cr\u00e9dit. Les sp\u00e9cialistes du marketing peuvent utiliser ces r\u00e8gles d'association pour organiser les produits corr\u00e9l\u00e9s plus pr\u00e8s les uns des autres sur les \u00e9tag\u00e8res des magasins ou faire des suggestions en ligne afin que les clients ach\u00e8tent plus d'articles. Un MBA peut g\u00e9n\u00e9ralement aider les d\u00e9taillants \u00e0 r\u00e9pondre aux questions les suivantes:\n\n- Quels articles sont souvent achet\u00e9s ensemble ?\n- \u00c9tant donn\u00e9 un panier, quels articles sugg\u00e9rer ?\n- Comment placer les articles ensemble sur les \u00e9tag\u00e8res ?\n\n### Objectif\n\nVotre objectif dans ce TP est de d\u00e9velopper un algorithme MBA pour r\u00e9v\u00e9ler les motifs en cr\u00e9ant des r\u00e8gles d'association dans un ensemble de donn\u00e9es volumineux avec plus de trois millions de transactions de supermarch\u00e9. Cependant, la collecte de r\u00e8gles d'association dans les grands ensembles de donn\u00e9es est un probl\u00e8me tr\u00e8s intensif en calcul, ce qui rend presque impossible leur ex\u00e9cution sans syst\u00e8me distribu\u00e9. Par cons\u00e9quent, pour ex\u00e9cuter votre algorithme, vous aurez acc\u00e8s \u00e0 un cluster de *cloud computing* distribu\u00e9 avec des centaines de c\u0153urs.\n\n\u00c0 cette fin, un algorithme **MapReduce** sera impl\u00e9ment\u00e9 avec le framework [Apache Spark](http://spark.apache.org), un syst\u00e8me informatique distribu\u00e9 rapide. En r\u00e9sum\u00e9, Spark est un framework open source con\u00e7u avec une m\u00e9thodologie *scale-out* qui en fait un outil tr\u00e8s puissant pour les programmeurs ou les d\u00e9veloppeurs d'applications pour effectuer un volume massif de calculs et de traitement de donn\u00e9es dans des environnements distribu\u00e9s. Spark fournit des API de haut niveau qui facilitent la cr\u00e9ation d'applications parall\u00e8les sans avoir \u00e0 se soucier de la fa\u00e7on dont votre code et vos donn\u00e9es sont parall\u00e9lis\u00e9s / distribu\u00e9s par le cluster informatique. Spark fait tout pour vous.\n\nLa mise en \u0153uvre suivra l'algorithme d'analyse du panier de march\u00e9 pr\u00e9sent\u00e9 par Jongwook Woo et Yuhang Xu (2012). L'image **workflow.pdf** illustre le flux de travail de l'algorithme et doit \u00eatre utilis\u00e9e pour consultation tout au long de ce TP. Les cases bleues sont celles o\u00f9 vous devez impl\u00e9menter une m\u00e9thode pour effectuer une fonction de mappage ou de r\u00e9duction, et les cases grises repr\u00e9sentent leur sortie attendue. **Toutes ces op\u00e9rations sont expliqu\u00e9es en d\u00e9tail dans les sections suivantes.**\n\n## 1. Configuration de Spark\n\nSpark fonctionne sur les syst\u00e8mes Windows et UNIX (par exemple, Linux, Mac OS). Il est facile d'ex\u00e9cuter Spark localement sur une seule machine - tout ce dont vous avez besoin est d'avoir Java install\u00e9 sur votre syst\u00e8me PATH, ou la variable d'environnement JAVA_HOME pointant vers une installation Java. Il est obligatoire que le **JDK v8** soit install\u00e9 sur votre syst\u00e8me, car Spark ne prend actuellement en charge que cette version. Si ce n'est pas le cas, acc\u00e9dez \u00e0 [la page Web de Java](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) pour t\u00e9l\u00e9charger et installer une machine virtuelle Java. N'oubliez pas de d\u00e9finir la variable d'environnement JAVA_HOME pour utiliser JDK v8 si votre installation ne le fait pas automatiquement.\n\nL'interface entre Python et Spark se fait via **PySpark**, qui peut \u00eatre install\u00e9 en ex\u00e9cutant `pip install pyspark` ou configur\u00e9 en suivant la s\u00e9quence ci-dessous:\n\n1. D'abord, allez sur http://spark.apache.org/downloads\n2. S\u00e9lectionnez la derni\u00e8re version de Spark et le package pr\u00e9-construit pour Apache Hadoop 2.7\n3. Cliquez pour t\u00e9l\u00e9charger **spark-2.4.5-bin-hadoop2.7.tgz** et d\u00e9compressez-le dans le dossier de votre choix.\n4. Ensuite, exportez les variables suivantes pour lier PYSPARK (l'interface python de Spark) \u00e0 votre distribution python dans votre fichier `~/.bash_profile`.\n\n``\nexport SPARK_HOME=/chemin/ vers / spark-2.4.5-bin-hadoop2.7\nexport PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$ PYTHONPATH\"\nexport PYSPARK_PYTHON=/chemin/vers/votre/python3\n``\n\n5. Ex\u00e9cutez `source ~./bash_profile` pour effectuer les modifications et red\u00e9marrer cette session de notebook jupyter.\n\n#### Alternative pour utiliser Google Colab\n\nSi vous pr\u00e9voyez d'utiliser la plateforme Google Colab, ex\u00e9cutez la cellule de code suivante pour configurer Spark:"}, {"cell_type": "code", "execution_count": 23, "metadata": {"id": "JmUMt4htMazm"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.1.3)\nRequirement already satisfied: py4j==0.10.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "import os\n!apt-get install openjdk-11-jdk-headless -qq > /dev/null\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n!pip install pyspark"}, {"cell_type": "markdown", "metadata": {"id": "rovSCW_vYs7m"}, "source": "#### Testez votre Spark\n\u00c0 l'aide du code suivant, vous pouvez tester si Spark est install\u00e9 correctement."}, {"cell_type": "code", "execution_count": 24, "metadata": {"id": "UxgNiBFkYs7n"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+\n|hello|\n+-----+\n|spark|\n+-----+\n\n"}], "source": "import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.sql(\"select 'spark' as hello \")\ndf.show()"}, {"cell_type": "markdown", "metadata": {"id": "QaYpUpOFMazu"}, "source": "### 1.1 Exemple de comptage de produits \n\nPour tester votre installation et commencer \u00e0 vous familiariser avec Spark, nous suivrons un exemple qui compte combien de fois les produits d'un toy dataset ont \u00e9t\u00e9 achet\u00e9s.\n\nLe principal point d'entr\u00e9e pour commencer la programmation avec Spark est [l'API RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html), une excellente abstraction Spark pour travailler avec MapReduce. RDD est une collection d'\u00e9l\u00e9ments partitionn\u00e9s sur les n\u0153uds du cluster qui peuvent fonctionner en parall\u00e8le. En d'autres termes, RDD est la fa\u00e7on dont Spark maintient vos donn\u00e9es pr\u00eates \u00e0 ex\u00e9cuter une fonction (par exemple, une fonction Map ou une fonction reduce) en parall\u00e8le. **Ne vous inqui\u00e9tez pas si cela semble toujours d\u00e9routant, il sera clair une fois que vous commencerez \u00e0 l'impl\u00e9menter**. Cependant, cela fait partie de ce TP d'\u00e9tudier / consulter [Spark python API](https://spark.apache.org/docs/latest/api/python/) et d'apprendre \u00e0 l'utiliser. Certaines fonctions utiles offertes par l'API RDD sont:\n\n1. **map**: return a new RDD by applying a function to each element of this RDD.\n2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n5. **groupByKey**: group the values for each key in the RDD into a single sequence\n6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n7. **sample**: return a sampled subset of this RDD\n8. **count**: return the number of elements in this RDD.\n9. **filter**: return a new RDD containing only the elements that satisfy a predicate."}, {"cell_type": "code", "execution_count": 30, "metadata": {"id": "NZDz1nrBMazu"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Toy dataset\n+--------+-----------+\n|order_id|transaction|\n+--------+-----------+\n|       1|    a;b;c;f|\n|       2|    d;b;a;e|\n|       3|        c;b|\n|       4|        b;c|\n+--------+-----------+\n\nToy dataframe as a RDD object (list of Row objects):\n\t [Row(order_id='1', transaction='a;b;c;f'), Row(order_id='2', transaction='d;b;a;e'), Row(order_id='3', transaction='c;b'), Row(order_id='4', transaction='b;c')]\n\nMapped products:\n\t [('a', 1), ('b', 1), ('c', 1), ('f', 1), ('d', 1), ('b', 1), ('a', 1), ('e', 1), ('c', 1), ('b', 1), ('b', 1), ('c', 1)]\n\nReduced (merged) products:\n\t [('a', 2), ('b', 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]\n\nVisualizing as a dataframe:\n+-------+-------------+\n|product|count_product|\n+-------+-------------+\n|      a|            2|\n|      b|            4|\n|      c|            3|\n|      f|            1|\n|      d|            1|\n|      e|            1|\n+-------+-------------+\n\n"}], "source": "from pyspark.sql import SparkSession\n\ndef map_to_product(row):\n    \"\"\"\n    Map each transaction into a set of KEY-VALUE elements.\n    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n    \"\"\"\n    products = row.transaction.split(';') # split products from the column transaction\n    for p in products:\n        yield (p, 1)\n\ndef reduce_product_by_key(value1, value2):\n    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n    return value1+value2\n\n# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\nspark = SparkSession.builder.getOrCreate()\n        \n# Read a toy dataset\ntoy = spark.read.csv('gs://my_bucket_inf8111_e23_tp2/toy.csv', header=True)\nprint(\"Toy dataset\")\ntoy.show()\n\n# Obtain a RDD object to call a map function\ntoy_rdd = toy.rdd\nprint(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n\n# Map function to identify all products\ntoy_rdd = toy_rdd.flatMap(map_to_product)\nprint(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n\n# Reduce function to merge values of elements that share the same KEY\ntoy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\nprint(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n\nprint(\"\\nVisualizing as a dataframe:\")\ntoy_rdd.toDF([\"product\", \"count_product\"]).show()"}, {"cell_type": "markdown", "metadata": {"id": "PpJGQmzXMazz"}, "source": "### 1.2 Travailler avec Spark Dataframe\n\nDans l'exemple ci-dessus, nous avons bri\u00e8vement utilis\u00e9 une classe Dataframe de Spark, mais uniquement pour obtenir un objet RDD avec ``toy.rdd`` et pour aficher les donn\u00e9es sous forme de tableau structur\u00e9 avec le ``show ()`` une fonction. Cependant, [Dataframe](http://spark.apache.org/docs/latest/api/python/) est une partie cruciale de la version actuelle de Spark et est construit sur l'API RDD. Il s'agit d'une collection distribu\u00e9e de lignes sous des colonnes nomm\u00e9es, identique \u00e0 une table dans une base de donn\u00e9es relationnelle. Le Dataframe de Spark fonctionne de la m\u00eame mani\u00e8re que [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). En fait, nous pouvons exporter (obtenir) une Dataframe Spark vers (\u00e0 partir de) \u200b\u200bune Dataframe pandas avec la fonction ``toPandas()``  (``spark.createDataFrame``).\n\nUne fonctionnalit\u00e9 centrale du Dataframe est de b\u00e9n\u00e9ficier du [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), un module qui permet des requ\u00eates SQL sur des donn\u00e9es structur\u00e9es. Par exemple, le m\u00eame \u00ab exemple de comptage de produits \u00bb aurait pu \u00eatre impl\u00e9ment\u00e9 comme une s\u00e9quence d'op\u00e9rations SQL sur les donn\u00e9es:"}, {"cell_type": "code", "execution_count": 31, "metadata": {"id": "oFL6BuIDMaz0"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "New column 'products': exploding the transaction's products to a new row\n+--------+-----------+--------+\n|order_id|transaction|products|\n+--------+-----------+--------+\n|       1|    a;b;c;f|       a|\n|       1|    a;b;c;f|       b|\n|       1|    a;b;c;f|       c|\n|       1|    a;b;c;f|       f|\n|       2|    d;b;a;e|       d|\n|       2|    d;b;a;e|       b|\n|       2|    d;b;a;e|       a|\n|       2|    d;b;a;e|       e|\n|       3|        c;b|       c|\n|       3|        c;b|       b|\n|       4|        b;c|       b|\n|       4|        b;c|       c|\n+--------+-----------+--------+\n\nCouting unique products:\n+--------+-------------+\n|products|count_product|\n+--------+-------------+\n|       b|            4|\n|       c|            3|\n|       a|            2|\n|       d|            1|\n|       e|            1|\n|       f|            1|\n+--------+-------------+\n\n"}], "source": "import pyspark.sql.functions as f\n\n# Creates a new column, products, with all products appering in each transaction\nprint('New column \\'products\\': exploding the transaction\\'s products to a new row')\ndf_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\ndf_toy.show()\n\n# Performs a select query and group rows by the product name, aggreagating by counting\nprint('Couting unique products:')\ndf_toy.select(df_toy.products)\\\n      .groupBy(df_toy.products)\\\n      .agg(f.count('products').alias('count_product'))\\\n      .sort('count_product', ascending=False)\\\n      .show()"}, {"cell_type": "markdown", "metadata": {"id": "W4HFs8CVMaz3"}, "source": "En outre, les m\u00eames op\u00e9rations SQL effectu\u00e9es ci-dessus auraient pu \u00eatre effectu\u00e9es avec une requ\u00eate en langage SQL traditionnel comme indiqu\u00e9 ci-dessous:"}, {"cell_type": "code", "execution_count": 32, "metadata": {"id": "O_eYl-7tMaz3"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+-------------+\n|products|product_count|\n+--------+-------------+\n|       b|            4|\n|       c|            3|\n|       a|            2|\n|       e|            1|\n|       d|            1|\n|       f|            1|\n+--------+-------------+\n\n"}], "source": "# Creates a relational table TOY in the Spark session\ndf_toy.createOrReplaceTempView(\"TOY\")\n\nspark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n          \" FROM TOY t\"\n          \" GROUP BY t.products\"\n          \" ORDER BY product_count DESC\").show()"}, {"cell_type": "markdown", "metadata": {"id": "T0Onr2NxMaz8"}, "source": "Ces concepts SQL sont mentionn\u00e9s ici car ils nous seront utiles lors du TP, principalement dans la section 3, pour manipuler les donn\u00e9es du supermarch\u00e9, qui sont structur\u00e9es en tableaux. Ainsi, si vous n'\u00eates pas familier avec SQL, il est recommand\u00e9 de suivre un [tutoriel](https://www.w3schools.com/sql/) pour comprendre les bases.\n\n## 2. Algorithme MBA\nLes sections suivantes expliquent comment d\u00e9velopper chaque \u00e9tape de l'algorithme MapReduce pour notre application de supermarch\u00e9. La figure workflow.pdf illustre chaque \u00e9tape de l'algorithme.\n\n### 2.1 Map to Patterns (10 points)\nPour un sous-ensemble de transactions (c'est-\u00e0-dire les lignes de notre toy dataset), chaque transaction doit \u00eatre **mapp\u00e9e** vers un ensemble de *motifs d'achat* trouv\u00e9s dans la transaction. Formellement, ces motifs sont des sous-ensembles de produits qui repr\u00e9sentent un groupe d'articles achet\u00e9s ensemble. \n\nPour le framework MapReduce, chaque motif doit \u00eatre cr\u00e9\u00e9 comme un \u00e9l\u00e9ment *KEY-VALUE*, o\u00f9 la KEY peut prendre la forme d'un singleton, d'une paire ou d'un trio de produits pr\u00e9sents dans la transaction. Plus pr\u00e9cis\u00e9ment, pour chaque transaction, la fonction de mappage doit g\u00e9n\u00e9rer tous les sous-ensembles **UNIQUE** possibles de taille **UN, DEUX ou TROIS**. La VALEUR associ\u00e9e \u00e0 chaque KEY est le nombre de fois que la KEY est apparue dans la transaction (si nous supposons qu'aucun produit n'appara\u00eet plus d'une fois dans la transaction, cette valeur est toujours \u00e9gale \u00e0 un).\n\nMaintenant, impl\u00e9mentez la fonction **map_to_patterns** qui re\u00e7oit une transaction (une ligne du dataset) et retourne les motifs trouv\u00e9s dans la transaction. Les \u00e9l\u00e9ments mapp\u00e9s sont un tuple (KEY, VALUE), o\u00f9 KEY est \u00e9galement un tuple de noms de produits. Il est crucial de noter que, puisque chaque entr\u00e9e (transaction) de la fonction MAP produira **plus** un \u00e9l\u00e9ment KEY-VALUE, un *flatMap* doit \u00eatre invoqu\u00e9 pour cette \u00e9tape.\n\nPour le toy dataset, la sortie attendue est similaire \u00e0:\n\n\n<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n<code>\n+---------------+-----------+\n|       patterns|occurrences|\n+---------------+-----------+\n|         ('a',)|          1|\n|     ('a', 'b')|          1|\n|('a', 'b', 'c')|          1|\n|('a', 'b', 'f')|          1|\n|     ('a', 'c')|          1|\n|('a', 'c', 'f')|          1|\n|     ('a', 'f')|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|('b', 'c', 'f')|          1|\n|     ('b', 'f')|          1|\n|         ('c',)|          1|\n|     ('c', 'f')|          1|\n|         ('f',)|          1|\n|         ('a',)|          1|\n|     ('a', 'b')|          1|\n|('a', 'b', 'd')|          1|\n|('a', 'b', 'e')|          1|\n|     ('a', 'd')|          1|\n|('a', 'd', 'e')|          1|\n|     ('a', 'e')|          1|\n|         ('b',)|          1|\n|     ('b', 'd')|          1|\n|('b', 'd', 'e')|          1|\n|     ('b', 'e')|          1|\n|         ('d',)|          1|\n|     ('d', 'e')|          1|\n|         ('e',)|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|         ('c',)|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|         ('c',)|          1|\n+---------------+-----------+\n</code>\n</pre>"}, {"cell_type": "code", "execution_count": 69, "metadata": {"id": "BscKKDAjMaz9"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+-----------+\n|       patterns|occurrences|\n+---------------+-----------+\n|         ('a',)|          1|\n|     ('a', 'b')|          1|\n|('a', 'b', 'c')|          1|\n|('a', 'b', 'f')|          1|\n|     ('a', 'c')|          1|\n|('a', 'c', 'f')|          1|\n|     ('a', 'f')|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|('b', 'c', 'f')|          1|\n|     ('b', 'f')|          1|\n|         ('c',)|          1|\n|     ('c', 'f')|          1|\n|         ('f',)|          1|\n|         ('a',)|          1|\n|     ('a', 'b')|          1|\n|('a', 'b', 'd')|          1|\n|('a', 'b', 'e')|          1|\n|     ('a', 'd')|          1|\n|('a', 'd', 'e')|          1|\n|     ('a', 'e')|          1|\n|         ('b',)|          1|\n|     ('b', 'd')|          1|\n|('b', 'd', 'e')|          1|\n|     ('b', 'e')|          1|\n|         ('d',)|          1|\n|     ('d', 'e')|          1|\n|         ('e',)|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|         ('c',)|          1|\n|         ('b',)|          1|\n|     ('b', 'c')|          1|\n|         ('c',)|          1|\n+---------------+-----------+\n\n"}], "source": "from itertools import combinations\nimport pandas as pd\n\ndef format_tuples(pattern):\n    \"\"\"\n    Used for visualizition.\n    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n    (a,b,c) -> '(a,b,c)'\n    \"\"\"\n    return (str(pattern[0]), str(pattern[1]))\n\ndef map_to_patterns(row):\n    \"\"\"\n    Map unique combinations of products for a single line.\n    \"\"\"\n    products = sorted(row.transaction.split(';'))\n    combinations_list = []\n    for p in range(1, 4):\n        order_combinations = list(combinations(products, p))\n        combinations_list.extend(order_combinations)\n        \n    # Tri ici pour avoir le m\u00eame ordre que l'exemple\n    combinations_list = sorted(combinations_list)\n    \n    for c in combinations_list:\n        yield (c, 1)\n\ntoy_rdd = toy.rdd\npatterns_rdd = toy_rdd.flatMap(map_to_patterns)\n\n# Output as dataframe\npatterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(50)"}, {"cell_type": "markdown", "metadata": {"id": "YvvRw0plMa0B"}, "source": "### 2.2  Reduce patterns  (2,5 points)\nUne fois que diff\u00e9rents processeurs ont trait\u00e9 les transactions, une fonction **reduce** doit \u00eatre appel\u00e9e pour combiner des KEYS identiques (le sous-ensemble de produits) et calculer le nombre total de ses occurrences dans le dataset. En d'autres termes, cette proc\u00e9dure de r\u00e9duction doit additionner la *VALUE* de chaque KEY identique.\n\nCr\u00e9ez ci-dessous une fonction **reduce_patterns** qui doit additionner la VALUE de chaque motif.\nPour le toy dataset, la sortie attendue est:\n<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n<code>\n+---------------+--------------------+\n|       patterns|combined_occurrences|\n+---------------+--------------------+\n|         ('a',)|                   2|\n|     ('a', 'b')|                   2|\n|('a', 'b', 'c')|                   1|\n|('a', 'b', 'f')|                   1|\n|     ('a', 'c')|                   1|\n|('a', 'c', 'f')|                   1|\n|     ('a', 'f')|                   1|\n|         ('b',)|                   4|\n|     ('b', 'c')|                   3|\n|('b', 'c', 'f')|                   1|\n|     ('b', 'f')|                   1|\n|         ('c',)|                   3|\n|     ('c', 'f')|                   1|\n|         ('f',)|                   1|\n|('a', 'b', 'd')|                   1|\n|('a', 'b', 'e')|                   1|\n|     ('a', 'd')|                   1|\n|('a', 'd', 'e')|                   1|\n|     ('a', 'e')|                   1|\n|     ('b', 'd')|                   1|\n|('b', 'd', 'e')|                   1|\n|     ('b', 'e')|                   1|\n|         ('d',)|                   1|\n|     ('d', 'e')|                   1|\n|         ('e',)|                   1|\n+---------------+--------------------+\n</code>\n</pre>\n"}, {"cell_type": "code", "execution_count": 70, "metadata": {"id": "67IKY_4MMa0C"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+--------------------+\n|       patterns|combined_occurrences|\n+---------------+--------------------+\n|         ('a',)|                   2|\n|     ('a', 'b')|                   2|\n|('a', 'b', 'c')|                   1|\n|('a', 'b', 'f')|                   1|\n|     ('a', 'c')|                   1|\n|('a', 'c', 'f')|                   1|\n|     ('a', 'f')|                   1|\n|         ('b',)|                   4|\n|     ('b', 'c')|                   3|\n|('b', 'c', 'f')|                   1|\n|     ('b', 'f')|                   1|\n|         ('c',)|                   3|\n|     ('c', 'f')|                   1|\n|         ('f',)|                   1|\n|('a', 'b', 'd')|                   1|\n|('a', 'b', 'e')|                   1|\n|     ('a', 'd')|                   1|\n|('a', 'd', 'e')|                   1|\n|     ('a', 'e')|                   1|\n|     ('b', 'd')|                   1|\n|('b', 'd', 'e')|                   1|\n|     ('b', 'e')|                   1|\n|         ('d',)|                   1|\n|     ('d', 'e')|                   1|\n|         ('e',)|                   1|\n+---------------+--------------------+\n\n"}], "source": "def reduce_patterns(value1, value2):\n    \"\"\"\n    Reduce the mapped objects to unique words\n    by merging (summing ) their values.\n    As done in exemple 1.1\n    \"\"\"\n    return value1+value2\n\n\ncombined_patterns_rdd = patterns_rdd.reduceByKey(reduce_product_by_key)\n\n# Output as dataframe\ncombined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show(25)"}, {"cell_type": "markdown", "metadata": {"id": "6BME1VugMa0F"}, "source": "### 2.3 Map to subpatterns (15 points)\nEnsuite, une autre fonction **map** doit \u00eatre appliqu\u00e9e pour g\u00e9n\u00e9rer des sous-motifs. Encore une fois, les sous-motifs sont des \u00e9l\u00e9ments KEY-VALUE, o\u00f9 la KEY est \u00e9galement un sous-ensemble de produits. Cependant, la cr\u00e9ation de la KEY du sous-motif est une proc\u00e9dure diff\u00e9rente. Cette fois, l'id\u00e9e est de d\u00e9composer la liste des produits de chaque motif (KEY), de supprimer un produit \u00e0 la fois et de produire la liste r\u00e9sultante en tant que nouvelle cl\u00e9 de sous-motif.\n\nPar exemple, pour un mod\u00e8le donn\u00e9 $P$ avec trois produits, $p_1, p_2$ et $p_3$, trois nouvelles cl\u00e9s de sous-motifs vont \u00eatre cr\u00e9\u00e9es: (i) supprimer $p_1$ et retourner ($p_2, p_3$) ; (ii) supprimer $p_2$ et retourner ($p_1, p_3$); et (iii) supprimer $p_3$ et retourner ($p_1, p_2$).\n\nDe plus, la structure VALUE du sous-motif sera \u00e9galement diff\u00e9rente. Au lieu d'une seule valeur enti\u00e8re unique comme nous l'avons eu dans les motifs, cette fois un *tuple* devrait \u00eatre cr\u00e9\u00e9 pour le sous-motif VALUE. Ce tuple contient le produit qui a \u00e9t\u00e9 retir\u00e9 lors de la remise de la KEY et le nombre de fois que le motif est apparu. Par exemple ci-dessus, les valeurs doivent \u00eatre ($p_1,v$), ($p_2,v$) et ($p_3,v $), respectivement, o\u00f9 $v$ est la VALEUR du motif.\n\nL'id\u00e9e derri\u00e8re les sous-motif est de cr\u00e9er **des r\u00e8gles** telles que : lorsque les produits de KEY ont \u00e9t\u00e9 achet\u00e9s, l'article pr\u00e9sent dans la VALEUR a \u00e9galement \u00e9t\u00e9 achet\u00e9 *v* fois. En outre, chaque motif doit \u00e9galement produire un sous-motif dans lequel la cl\u00e9 est la m\u00eame liste de produits du motif, mais la valeur est un tuple avec un produit nul (None) et le nombre de fois que le motif est apparu. Cet \u00e9l\u00e9ment sera utile pour garder une trace du nombre de fois o\u00f9 un tel motif a \u00e9t\u00e9 trouv\u00e9 et sera utilis\u00e9 ult\u00e9rieurement pour calculer la valeur de confiance lors de la g\u00e9n\u00e9ration des r\u00e8gles d'association.\n\nMaintenant, impl\u00e9mentez la fonction **map_to_subpatterns** qui re\u00e7oit un motif et produit tous les sous-motif trouv\u00e9s. Encore une fois, chaque entr\u00e9e (motif) g\u00e9n\u00e9rera plus d'un \u00e9l\u00e9ment KEY-VALUE, puis une fonction flatMap doit \u00eatre appel\u00e9e.\n\nPour le toy dataset, la sortie attendue est:\n\n<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n<code>\n+---------------+---------+\n|    subpatterns|    rules|\n+---------------+---------+\n|         ('a',)|(None, 2)|\n|     ('a', 'b')|(None, 2)|\n|         ('b',)| ('a', 2)|\n|         ('a',)| ('b', 2)|\n|('a', 'b', 'c')|(None, 1)|\n|     ('b', 'c')| ('a', 1)|\n|     ('a', 'c')| ('b', 1)|\n|     ('a', 'b')| ('c', 1)|\n|('a', 'b', 'f')|(None, 1)|\n|     ('b', 'f')| ('a', 1)|\n|     ('a', 'f')| ('b', 1)|\n|     ('a', 'b')| ('f', 1)|\n|     ('a', 'c')|(None, 1)|\n|         ('c',)| ('a', 1)|\n|         ('a',)| ('c', 1)|\n|('a', 'c', 'f')|(None, 1)|\n|     ('c', 'f')| ('a', 1)|\n|     ('a', 'f')| ('c', 1)|\n|     ('a', 'c')| ('f', 1)|\n|     ('a', 'f')|(None, 1)|\n|         ('f',)| ('a', 1)|\n|         ('a',)| ('f', 1)|\n|         ('b',)|(None, 4)|\n|     ('b', 'c')|(None, 3)|\n|         ('c',)| ('b', 3)|\n|         ('b',)| ('c', 3)|\n|('b', 'c', 'f')|(None, 1)|\n|     ('c', 'f')| ('b', 1)|\n|     ('b', 'f')| ('c', 1)|\n|     ('b', 'c')| ('f', 1)|\n|     ('b', 'f')|(None, 1)|\n|         ('f',)| ('b', 1)|\n|         ('b',)| ('f', 1)|\n|         ('c',)|(None, 3)|\n|     ('c', 'f')|(None, 1)|\n|         ('f',)| ('c', 1)|\n|         ('c',)| ('f', 1)|\n|         ('f',)|(None, 1)|\n|('a', 'b', 'd')|(None, 1)|\n|     ('b', 'd')| ('a', 1)|\n|     ('a', 'd')| ('b', 1)|\n|     ('a', 'b')| ('d', 1)|\n|('a', 'b', 'e')|(None, 1)|\n|     ('b', 'e')| ('a', 1)|\n|     ('a', 'e')| ('b', 1)|\n|     ('a', 'b')| ('e', 1)|\n|     ('a', 'd')|(None, 1)|\n|         ('d',)| ('a', 1)|\n|         ('a',)| ('d', 1)|\n|('a', 'd', 'e')|(None, 1)|\n|     ('d', 'e')| ('a', 1)|\n|     ('a', 'e')| ('d', 1)|\n|     ('a', 'd')| ('e', 1)|\n|     ('a', 'e')|(None, 1)|\n|         ('e',)| ('a', 1)|\n|         ('a',)| ('e', 1)|\n|     ('b', 'd')|(None, 1)|\n|         ('d',)| ('b', 1)|\n|         ('b',)| ('d', 1)|\n|('b', 'd', 'e')|(None, 1)|\n|     ('d', 'e')| ('b', 1)|\n|     ('b', 'e')| ('d', 1)|\n|     ('b', 'd')| ('e', 1)|\n|     ('b', 'e')|(None, 1)|\n|         ('e',)| ('b', 1)|\n|         ('b',)| ('e', 1)|\n|         ('d',)|(None, 1)|\n|     ('d', 'e')|(None, 1)|\n|         ('e',)| ('d', 1)|\n|         ('d',)| ('e', 1)|\n|         ('e',)|(None, 1)|\n+---------------+---------+\n</code>\n</pre>"}, {"cell_type": "code", "execution_count": 79, "metadata": {"id": "t8aLrdMuMa0G"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+---------+\n|    subpatterns|    rules|\n+---------------+---------+\n|         ('a',)|(None, 2)|\n|     ('a', 'b')|(None, 2)|\n|         ('b',)| ('a', 2)|\n|         ('a',)| ('b', 2)|\n|('a', 'b', 'c')|(None, 1)|\n|     ('b', 'c')| ('a', 1)|\n|     ('a', 'c')| ('b', 1)|\n|     ('a', 'b')| ('c', 1)|\n|('a', 'b', 'f')|(None, 1)|\n|     ('b', 'f')| ('a', 1)|\n|     ('a', 'f')| ('b', 1)|\n|     ('a', 'b')| ('f', 1)|\n|     ('a', 'c')|(None, 1)|\n|         ('c',)| ('a', 1)|\n|         ('a',)| ('c', 1)|\n|('a', 'c', 'f')|(None, 1)|\n|     ('c', 'f')| ('a', 1)|\n|     ('a', 'f')| ('c', 1)|\n|     ('a', 'c')| ('f', 1)|\n|     ('a', 'f')|(None, 1)|\n|         ('f',)| ('a', 1)|\n|         ('a',)| ('f', 1)|\n|         ('b',)|(None, 4)|\n|     ('b', 'c')|(None, 3)|\n|         ('c',)| ('b', 3)|\n|         ('b',)| ('c', 3)|\n|('b', 'c', 'f')|(None, 1)|\n|     ('c', 'f')| ('b', 1)|\n|     ('b', 'f')| ('c', 1)|\n|     ('b', 'c')| ('f', 1)|\n|     ('b', 'f')|(None, 1)|\n|         ('f',)| ('b', 1)|\n|         ('b',)| ('f', 1)|\n|         ('c',)|(None, 3)|\n|     ('c', 'f')|(None, 1)|\n|         ('f',)| ('c', 1)|\n|         ('c',)| ('f', 1)|\n|         ('f',)|(None, 1)|\n|('a', 'b', 'd')|(None, 1)|\n|     ('b', 'd')| ('a', 1)|\n|     ('a', 'd')| ('b', 1)|\n|     ('a', 'b')| ('d', 1)|\n|('a', 'b', 'e')|(None, 1)|\n|     ('b', 'e')| ('a', 1)|\n|     ('a', 'e')| ('b', 1)|\n|     ('a', 'b')| ('e', 1)|\n|     ('a', 'd')|(None, 1)|\n|         ('d',)| ('a', 1)|\n|         ('a',)| ('d', 1)|\n|('a', 'd', 'e')|(None, 1)|\n|     ('d', 'e')| ('a', 1)|\n|     ('a', 'e')| ('d', 1)|\n|     ('a', 'd')| ('e', 1)|\n|     ('a', 'e')|(None, 1)|\n|         ('e',)| ('a', 1)|\n|         ('a',)| ('e', 1)|\n|     ('b', 'd')|(None, 1)|\n|         ('d',)| ('b', 1)|\n|         ('b',)| ('d', 1)|\n|('b', 'd', 'e')|(None, 1)|\n|     ('d', 'e')| ('b', 1)|\n|     ('b', 'e')| ('d', 1)|\n|     ('b', 'd')| ('e', 1)|\n|     ('b', 'e')|(None, 1)|\n|         ('e',)| ('b', 1)|\n|         ('b',)| ('e', 1)|\n|         ('d',)|(None, 1)|\n|     ('d', 'e')|(None, 1)|\n|         ('e',)| ('d', 1)|\n|         ('d',)| ('e', 1)|\n|         ('e',)|(None, 1)|\n+---------------+---------+\n\n"}], "source": "from copy import deepcopy\ndef map_to_subpatterns(pattern):\n    \"\"\"\n    Show the sub-patterns\n    \"\"\"\n    yield (pattern[0], tuple((None, pattern[1])))\n    if len(pattern[0]) > 1:\n        for item in pattern[0]:\n            yield(tuple(sub_pattern for sub_pattern in pattern[0] if sub_pattern != item), \n                  tuple((item, pattern[1])))\n\nsubpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n\n# Output as dataframe\nsubpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(100)"}, {"cell_type": "markdown", "metadata": {"id": "jl6TWh8rMa0J"}, "source": "### 2.4 Reduce Subpatterns (2.5 points)\n\nEncore une fois, une fonction **reduce** est n\u00e9cessaire pour regrouper tous les sous-motif par leur KEY. L'objectif de cette proc\u00e9dure de r\u00e9duction est de cr\u00e9er une liste de toutes les **r\u00e8gles** apparues dans KEY. Par cons\u00e9quent, la sortie attendue r\u00e9sultant de cette fonction de r\u00e9duction est \u00e9galement un \u00e9l\u00e9ment KEY-VALUE, o\u00f9 la cl\u00e9 est la KEY du sous-motif et la valeur est un groupe contenant toutes les valeurs des sous-motif qui partagent la m\u00eame cl\u00e9.\n\nPour le toy dataset, la sortie attendue est:\n\n\n<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n<code>\n+---------------+-------------------------------------------------------------+\n|subpatterns    |combined_rules                                               |\n+---------------+-------------------------------------------------------------+\n|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n|('a', 'b', 'c')|[(None, 1)]                                                  |\n|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n|('a', 'b', 'f')|[(None, 1)]                                                  |\n|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n|('a', 'c', 'f')|[(None, 1)]                                                  |\n|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n|('b', 'c', 'f')|[(None, 1)]                                                  |\n|('a', 'b', 'd')|[(None, 1)]                                                  |\n|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n|('a', 'b', 'e')|[(None, 1)]                                                  |\n|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n+---------------+-------------------------------------------------------------+\n</code>\n</pre>\n"}, {"cell_type": "code", "execution_count": 100, "metadata": {"id": "LOP-SVIhMa0J"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+-------------------------------------------------------------+\n|subpatterns    |combined_rules                                               |\n+---------------+-------------------------------------------------------------+\n|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n|('a', 'b', 'c')|[(None, 1)]                                                  |\n|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n|('a', 'b', 'f')|[(None, 1)]                                                  |\n|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n|('a', 'c', 'f')|[(None, 1)]                                                  |\n|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n|('b', 'c', 'f')|[(None, 1)]                                                  |\n|('a', 'b', 'd')|[(None, 1)]                                                  |\n|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n|('a', 'b', 'e')|[(None, 1)]                                                  |\n|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n+---------------+-------------------------------------------------------------+\nonly showing top 20 rows\n\n"}], "source": "combined_rules = subpatterns_rdd.groupByKey().mapValues(list)\n\n# Output as dataframe\ncombined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(truncate=False)"}, {"cell_type": "markdown", "metadata": {"id": "Uh69x3a8Ma0P"}, "source": "### 2.5. Map to Association Rules (15 points)\n\nEnfin, la derni\u00e8re \u00e9tape de l'algorithme consiste \u00e0 cr\u00e9er les r\u00e8gles d'association pour effectuer la MBA. Le but de cette fonction Map est de calculer le niveau **de confiance** de l'achat d'un produit, sachant qu'il y a d\u00e9j\u00e0 un ensemble de produits dans le panier. Ainsi, la KEY du sous-motif est l'ensemble des produits plac\u00e9s dans le panier et, pour chaque produit pr\u00e9sent dans la liste des r\u00e8gles, c'est-\u00e0-dire dans la VALEUR, la confiance peut \u00eatre calcul\u00e9e comme :\n\n\\begin{align*}\n\\frac{\\text{nombre de fois o\u00f9 le produit a \u00e9t\u00e9 achet\u00e9 avec KEY}}{\\text{nombre de fois o\u00f9 la KEY est apparue}}\n\\end{align*}\n\nPour l'exemple donn\u00e9 dans la figure \"workflow\", *le caf\u00e9* a \u00e9t\u00e9 achet\u00e9 20 fois et, dans 17 d'entre eux, le *lait* a \u00e9t\u00e9 achet\u00e9 ensemble. Ensuite, le niveau de confiance pour acheter du *lait* sachant que *le caf\u00e9* est dans le panier est $\\frac{17}{20}=0,85$, ce qui signifie que dans 85% des cas o\u00f9 le caf\u00e9 a \u00e9t\u00e9 achet\u00e9, le lait a aussi \u00e9t\u00e9 achet\u00e9.\n\nImpl\u00e9mentez la fonction **map_to_assoc_rules** qui calcule le niveau de confiance pour chaque sous-motif.\n\nPour le toy dataset, la sortie attendue est:\n<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n<code>\n+---------------+------------------------------------------------------------------+\n|patterns       |association_rules                                                 |\n+---------------+------------------------------------------------------------------+\n|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n|('a', 'b', 'c')|[]                                                                |\n|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n|('a', 'b', 'f')|[]                                                                |\n|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n|('a', 'c', 'f')|[]                                                                |\n|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n|('b', 'c', 'f')|[]                                                                |\n|('a', 'b', 'd')|[]                                                                |\n|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n|('a', 'b', 'e')|[]                                                                |\n|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n+---------------+------------------------------------------------------------------+\n</code>\n</pre>"}, {"cell_type": "code", "execution_count": 103, "metadata": {"id": "DPrbn5CfMa0P"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+------------------------------------------------------------+\n|patterns       |association_rules                                           |\n+---------------+------------------------------------------------------------+\n|('a',)         |['b', 1.0, 'c', 0.5, 'f', 0.5, 'd', 0.5, 'e', 0.5]          |\n|('a', 'b')     |['c', 0.5, 'f', 0.5, 'd', 0.5, 'e', 0.5]                    |\n|('b',)         |['a', 0.5, 'c', 0.75, 'f', 0.25, 'd', 0.25, 'e', 0.25]      |\n|('a', 'b', 'c')|[]                                                          |\n|('b', 'c')     |['a', 0.3333333333333333, 'f', 0.3333333333333333]          |\n|('a', 'c')     |['b', 1.0, 'f', 1.0]                                        |\n|('a', 'b', 'f')|[]                                                          |\n|('b', 'f')     |['a', 1.0, 'c', 1.0]                                        |\n|('a', 'f')     |['b', 1.0, 'c', 1.0]                                        |\n|('c',)         |['a', 0.3333333333333333, 'b', 1.0, 'f', 0.3333333333333333]|\n|('a', 'c', 'f')|[]                                                          |\n|('c', 'f')     |['a', 1.0, 'b', 1.0]                                        |\n|('f',)         |['a', 1.0, 'b', 1.0, 'c', 1.0]                              |\n|('b', 'c', 'f')|[]                                                          |\n|('a', 'b', 'd')|[]                                                          |\n|('b', 'd')     |['a', 1.0, 'e', 1.0]                                        |\n|('a', 'd')     |['b', 1.0, 'e', 1.0]                                        |\n|('a', 'b', 'e')|[]                                                          |\n|('b', 'e')     |['a', 1.0, 'd', 1.0]                                        |\n|('a', 'e')     |['b', 1.0, 'd', 1.0]                                        |\n+---------------+------------------------------------------------------------+\nonly showing top 20 rows\n\n"}], "source": "def map_to_assoc_rules(rule):\n    \"\"\"\n    Map the association rules\n    \"\"\"\n    association_rules = []\n    \n    for i in rule[1]:\n        if i[0] is None:\n            key_occurences = i[1]\n    \n    for i in rule[1]:\n        if i[0] is not None:\n            association_rules.extend(tuple((i[0], i[1]/key_occurences)))\n            \n    yield(rule[0],association_rules)\n\nassoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n\n# Output as dataframe\nassoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(truncate=False)"}, {"cell_type": "markdown", "metadata": {"id": "BPV5g2hwMa0U"}, "source": "## 3. Instacart dataset\n\nAvec votre algorithme MBA pr\u00eat \u00e0 \u00eatre utilis\u00e9, il est maintenant temps de travailler sur l'ensemble de donn\u00e9es r\u00e9el. Pour cette partie du TP, t\u00e9l\u00e9chargez le dataset [instacart](https://drive.google.com/file/d/1pXjqPz1RbL40yCGWnTCbmW_ZXrjlfJi4/view?usp=sharing) et lisez sa [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) pour comprendre la structure de l'ensemble de donn\u00e9es.\n\nAvant d'appliquer l'algorithme d\u00e9velopp\u00e9 sur l'ensemble de donn\u00e9es instacart, vous devez d'abord filtrer les transactions pour qu'elles soient au m\u00eame format d\u00e9fini par votre algorithme (une transaction par ligne). Pour manipuler les donn\u00e9es, nous pouvons utiliser le bloc de donn\u00e9es de Spark et le module SQL pr\u00e9sent\u00e9 dans la section 1.\n\nLa cellule de code suivante utilise le module Spark SQL pour lire les commandes de ``order_products__train.csv`` et les informations d\u00e9taill\u00e9es de ``orders.csv`` et ``products.csv`` pour construire une dataframe qui contient un liste de tous les produits jamais achet\u00e9s par chaque utilisateur."}, {"cell_type": "code", "execution_count": 115, "metadata": {"id": "6oB1eTkeMa0W"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "order_products__train.csv\n+--------+----------+-----------------+---------+\n|order_id|product_id|add_to_cart_order|reordered|\n+--------+----------+-----------------+---------+\n|       1|     49302|                1|        1|\n|       1|     11109|                2|        1|\n|       1|     10246|                3|        0|\n|       1|     49683|                4|        0|\n|       1|     43633|                5|        1|\n+--------+----------+-----------------+---------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "orders.csv\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n| 2539329|      1|   prior|           1|        2|                8|                  null|\n| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n|  473747|      1|   prior|           3|        3|               12|                  21.0|\n| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n|  431534|      1|   prior|           5|        4|               15|                  28.0|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\nonly showing top 5 rows\n\nproducts.csv\n+----------+--------------------+--------+-------------+\n|product_id|        product_name|aisle_id|department_id|\n+----------+--------------------+--------+-------------+\n|         1|Chocolate Sandwic...|      61|           19|\n|         2|    All-Seasons Salt|     104|           13|\n|         3|Robust Golden Uns...|      94|            7|\n|         4|Smart Ones Classi...|      38|            1|\n|         5|Green Chile Anyti...|       5|           13|\n+----------+--------------------+--------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 259:=======>                                                 (1 + 7) / 8]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+--------------------------------------------------------------------------------+\n|user_id|                                                                        products|\n+-------+--------------------------------------------------------------------------------+\n|      1|[Zero Calorie Cola, Organic Half & Half, Organic Whole Milk, Aged White Chedd...|\n|      2|[Organic Hearty Split Pea & Uncured Ham Soup, Organic Cashew Carrot Ginger So...|\n|      5|[Organic Baby Arugula, Organic Grape Tomatoes, Tamari Gluten Free Soy Sauce, ...|\n|      7|[Honeycrisp Apple, Organic Dark Brown Sugar, Vanilla Coffee Concentrate, Lact...|\n|      8|[Organic Green Onions, Solid White-No Salt Added Albacore Tuna, Organic Whole...|\n+-------+--------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "cloud_path = 'gs://my_bucket_inf8111_e23_tp2/'\n\ndf_order_prod = spark.read.csv(cloud_path + 'instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\nprint('order_products__train.csv')\ndf_order_prod.show(5)\n\ndf_orders = spark.read.csv(cloud_path + 'instacart/orders.csv', header=True, sep=',', inferSchema=True)\nprint('orders.csv')\ndf_orders.show(5)\n\ndf_products = spark.read.csv(cloud_path + 'instacart/products.csv', header=True, sep=',', inferSchema=True)\nprint('products.csv')\ndf_products.show(5)\n\n\n\"\"\"\nList of products ever purchased by each user\n\"\"\"\n# USING SQL\ndf_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\ndf_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\ndf_products.createOrReplaceTempView(\"products\") # creates table 'products'\nspark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n               ' FROM orders o '\n               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n               ' INNER JOIN products p    ON op.product_id = p.product_id'\n               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n\n\n# USING DATAFRAME OPERATIONS\n# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n# .orderBy(df_orders.user_id).show(5, truncate=80)"}, {"cell_type": "markdown", "metadata": {"id": "JEqVeqhkMa0a"}, "source": "### 3.1 Perspectives commerciales (20 points) \n\nMaintenant, vous \u00eates le *data scientist*. En ne consid\u00e9rant que les commandes de ``order_products__train.csv``, l'utilisation du module Spark SQL, performant avec SQL ou dataframe, pour r\u00e9pondre aux questions suivantes:\n\n1. Quels sont les 10 produits les plus susceptibles d'\u00eatre command\u00e9 de nouveau? Ne consid\u00e9rez que les produits achet\u00e9s au moins 40 fois pour cette t\u00e2che.\n2. Quels sont les 3 produits les plus achet\u00e9s dans chaque d\u00e9partement?\n4. Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n    - utilisez un barplot pour visualiser vos r\u00e9sultats\n\n**La sortie de ces questions doit contenir le NOM des produits, pas leur ID.**"}, {"cell_type": "code", "execution_count": 247, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+---------+------------------+\n|        product_name|order_count|reordered|reorder_percentage|\n+--------------------+-----------+---------+------------------+\n|2% Lactose Free Milk|         92|       92|0.9347826086956522|\n|Organic Low Fat Milk|        368|      368|0.9130434782608695|\n|100% Florida Oran...|         59|       59|0.8983050847457628|\n|Organic Spelt Tor...|         81|       81|0.8888888888888888|\n|Original Sparklin...|         45|       45|0.8888888888888888|\n|              Banana|      18726|    18726|0.8841717398269785|\n|  Petit Suisse Fruit|        120|      120|0.8833333333333333|\n|Organic Lowfat 1%...|        483|      483|0.8819875776397516|\n|Organic Lactose F...|        269|      269|0.8810408921933085|\n|      1% Lowfat Milk|        461|      461|0.8785249457700651|\n+--------------------+-----------+---------+------------------+\nonly showing top 10 rows\n\n"}], "source": "Q1_sql = \"\"\"\nSELECT\n  products.product_name,\n  order_count,\n  reordered,\n  reorder_percentage\nFROM\n  (\n    SELECT\n      order_prod.product_id,\n      COUNT(order_prod.product_id) AS order_count,\n      COUNT(order_prod.reordered) AS reordered,\n      SUM(order_prod.reordered) / COUNT(order_prod.product_id) AS reorder_percentage\n    FROM\n      order_prod\n    GROUP BY\n      product_id\n  ) AS Table1\n  LEFT JOIN products ON Table1.product_id = products.product_id\nWHERE\n  order_count >= 40\nORDER BY\n  reorder_percentage DESC\n\"\"\"\n\nQ1_df = spark.sql(Q1_sql)\nQ1_df.show(10)\n"}, {"cell_type": "code", "execution_count": 248, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+-----------+-------+\n|        product_name|department_id|order_count|ranking|\n+--------------------+-------------+-----------+-------+\n|         Blueberries|            1|       2323|      1|\n|Organic Broccoli ...|            1|       1361|      2|\n|Organic Whole Str...|            1|       1213|      3|\n|         Dried Mango|           10|        446|      1|\n| Organic Rolled Oats|           10|        259|      2|\n|Organic Black Mis...|           10|        125|      3|\n|  Lavender Hand Soap|           11|        258|      1|\n|        Cotton Swabs|           11|        258|      1|\n|Lemon Verbena Han...|           11|        191|      3|\n|Boneless Skinless...|           12|       2088|      1|\n|Ground Turkey Breast|           12|        958|      2|\n|Boneless Skinless...|           12|        943|      3|\n|Extra Virgin Oliv...|           13|       2068|      1|\n|Creamy Peanut Butter|           13|        991|      2|\n|Creamy Almond Butter|           13|        850|      3|\n|  Honey Nut Cheerios|           14|       1218|      1|\n|Organic Old Fashi...|           14|        747|      2|\n|  Raisin Bran Cereal|           14|        600|      3|\n| Organic Black Beans|           15|       1576|      1|\n|No Salt Added Bla...|           15|       1250|      2|\n+--------------------+-------------+-----------+-------+\nonly showing top 20 rows\n\n"}], "source": "Q2_sql = \"\"\"\nSELECT *\nFROM(\nSELECT\n  products.product_name,\n  products.department_id,\n  order_count,\n  RANK() OVER (PARTITION BY products.department_id ORDER BY order_count DESC) AS ranking\nFROM\n  (\n    SELECT\n      order_prod.product_id,\n      COUNT(order_prod.product_id) AS order_count\n    FROM\n      order_prod\n    GROUP BY\n      product_id\n  ) AS Table1\n  LEFT JOIN products ON Table1.product_id = products.product_id\nORDER  BY department_id ASC, order_count DESC)\nWhere ranking < 4\n\"\"\"\n\nQ2_df = spark.sql(Q2_sql)\nQ2_df.show()"}, {"cell_type": "code", "execution_count": 269, "metadata": {}, "outputs": [{"ename": "ParseException", "evalue": "\nmismatched input 'GROUP' expecting {<EOF>, ';'}(line 17, pos 0)\n\n== SQL ==\n\nSELECT\n  order_dow,\n  Avg(items_ordered) AS Daily_Average_Basket\nFROM\n  (\n    SELECT\n      order_prod.order_id,\n      COUNT(order_prod.order_id) AS items_ordered\n    FROM\n      order_prod\n    GROUP BY\n      order_id\n  ) AS Table1\n  LEFT JOIN orders ON Table1.order_id = orders.order_id\nORDER BY order_dow\nGROUP BY order_dow\n^^^\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[269], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m Q3_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  order_dow,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mGROUP BY order_dow\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m Q3_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ3_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m Q3_df\u001b[38;5;241m.\u001b[39mshow()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mParseException\u001b[0m: \nmismatched input 'GROUP' expecting {<EOF>, ';'}(line 17, pos 0)\n\n== SQL ==\n\nSELECT\n  order_dow,\n  Avg(items_ordered) AS Daily_Average_Basket\nFROM\n  (\n    SELECT\n      order_prod.order_id,\n      COUNT(order_prod.order_id) AS items_ordered\n    FROM\n      order_prod\n    GROUP BY\n      order_id\n  ) AS Table1\n  LEFT JOIN orders ON Table1.order_id = orders.order_id\nORDER BY order_dow\nGROUP BY order_dow\n^^^\n"]}], "source": "Q3_sql = \"\"\"\nSELECT\n  order_dow,\n  Avg(items_ordered) AS Daily_Average_Basket\nFROM\n  (\n    SELECT\n      order_prod.order_id,\n      COUNT(order_prod.order_id) AS items_ordered\n    FROM\n      order_prod\n    GROUP BY\n      order_id\n  ) AS Table1\n  LEFT JOIN orders ON Table1.order_id = orders.order_id\nGROUP BY order_dow\nORDER BY order_dow\n\"\"\"\n\nQ3_df = spark.sql(Q3_sql)\nQ3_df.show()"}, {"cell_type": "markdown", "metadata": {"id": "PEWqTH1QMa0a"}, "source": "### 3.2 MBA pour le training set (15 points)\n\nEn utilisant les commandes du ``order_products__train.csv``, cr\u00e9ez un bloc de donn\u00e9es o\u00f9 chaque ligne contient la colonne ``transaction`` avec la liste des produits achet\u00e9s, de mani\u00e8re similaire \u00e0 le toy dataset. Ensuite, ex\u00e9cutez l'algorithme MBA pour cet ensemble de transactions.\n\n- Vous devez signaler le temps pass\u00e9 pour effectuer cette t\u00e2che.\n- La sortie doit contenir le nom des produits."}, {"cell_type": "code", "execution_count": 113, "metadata": {"id": "vxZh_f3hMa0b"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------------------------+-----------+------------------+\n|                         product_name|total_order|     proba_reorder|\n+-------------------------------------+-----------+------------------+\n|                 2% Lactose Free Milk|         92|0.9347826086956522|\n|                 Organic Low Fat Milk|        368|0.9130434782608695|\n|            100% Florida Orange Juice|         59|0.8983050847457628|\n|              Organic Spelt Tortillas|         81|0.8888888888888888|\n|Original Sparkling Seltzer Water Cans|         45|0.8888888888888888|\n|                               Banana|      18726|0.8841717398269785|\n|                   Petit Suisse Fruit|        120|0.8833333333333333|\n|               Organic Lowfat 1% Milk|        483|0.8819875776397516|\n|  Organic Lactose Free 1% Lowfat Milk|        269|0.8810408921933085|\n|                       1% Lowfat Milk|        461|0.8785249457700651|\n+-------------------------------------+-----------+------------------+\n\nCPU times: user 3.79 ms, sys: 665 \u00b5s, total: 4.45 ms\nWall time: 697 ms\n"}], "source": "%%time\n\"\"\"\nTODO: create a query to create and sctruct the transactions\n\"\"\"\n\nrequete_sql = spark.sql(\"\"\" SELECT p1.product_name, \n                              total_order, \n                              nbr_reorder / total_order AS proba_reorder \n                      FROM (SELECT product_id, \n                                   COUNT(product_id) AS total_order, \n                                   SUM(reordered) AS nbr_reorder FROM order_prod GROUP BY product_id) \n                      AS p2 INNER JOIN products AS p1 ON p1.product_id = p2.product_id \n                      WHERE total_order >= 40 \n                      ORDER BY proba_reorder DESC LIMIT 10 \"\"\")\n\ntop_10.show(15, truncate=80)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "dewN0YUEMa0h"}, "outputs": [], "source": "%%time\n\"\"\"\nTODO: run the MBA algorithm and show the first 5 association rules\n\"\"\"\n"}, {"cell_type": "markdown", "metadata": {"id": "5DkNPEtGMa0l"}, "source": "# 3.3 MBA pour le dataset complet (20 points)\n\nComme vous l'avez probablement remarqu\u00e9, m\u00eame pour un ensemble de donn\u00e9es moins volumineux (le training dataset ne contient que 131 000 commandes), l'algorithme MBA est co\u00fbteux en calcul. Pour cette raison, cette fois, nous allons r\u00e9p\u00e9ter le processus, mais en utilisant maintenant Google Cloud Platform (GCP) pour cr\u00e9er un grand cluster. Toutes les instructions pour cr\u00e9er un cluster avec spark et comment soumettre un travail seront expliqu\u00e9es dans le laboratoire. Dans tous les cas, vous devez lire les instructions donn\u00e9es dans le ``Instruction_GCP.pdf``.\n\nCette fois, nous travaillerons avec le fichier ``order_products__prior.csv``, qui contient plus de 3M commandes.\n\n**PRODUCTION ATTENDUE**\n\nApr\u00e8s avoir ex\u00e9cut\u00e9 le MBA pour la plus grande collection de commandes, s\u00e9lectionnez au hasard UN produit achet\u00e9 dans ``order_products__prior`` et affichez les r\u00e8gles d'association (nom du produit et valeur d'association) de ce produit, c'est-\u00e0-dire lorsque le produit est seul dans le panier. La sortie doit \u00eatre format\u00e9e dans un tableau, o\u00f9 chaque ligne contenant les informations d'un produit associ\u00e9. \n\n- Affichez l'ID et le nom du produit s\u00e9lectionn\u00e9 au hasard.\n- Signaler le temps d'ex\u00e9cution.\n\n**Remarque importante\u00a0: joignez des captures d'\u00e9cran de votre sortie et de votre configuration de cluster.** "}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "X1cVWxraMa0l"}, "outputs": [], "source": "%%time\n\"\"\"\nTODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n\"\"\"\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "MVBQ12b2Ma0o"}, "outputs": [], "source": "%%time\n\"\"\"\nTODO: run the MBA algorithm and print the requested output\n\"\"\"\n"}], "metadata": {"colab": {"collapsed_sections": [], "name": "tp2.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}